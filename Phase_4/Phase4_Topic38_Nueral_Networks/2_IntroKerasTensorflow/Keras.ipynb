{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div style=\"color:white;\n",
    "           display:fill;\n",
    "           border-radius:5px;\n",
    "           background-color:#5642C5;\n",
    "           font-size:200%;\n",
    "           font-family:Arial;letter-spacing:0.5px\">\n",
    "\n",
    "<p width = 20%, style=\"padding: 10px;\n",
    "              color:white;\">\n",
    "Neural Networks: Keras and Tensorflow\n",
    "</p>\n",
    "</div>\n",
    "\n",
    "Data Science Cohort Live NYC July 2022\n",
    "<p>Phase 4: Topic 40</p>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<div align = \"right\">\n",
    "<img src=\"Images/flatiron-school-logo.png\" align = \"right\" width=\"200\"/>\n",
    "</div>\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join(os.pardir, os.pardir))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_digits, load_sample_images\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "import seaborn as sns\n",
    "    \n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Neural Networks: Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "When manually implementing:\n",
    "\n",
    "- have to keep track of forward and backwards propagation for each layer:\n",
    "    - equations and updates depends on specific layer activations\n",
    "- manage caching updates to gradients and weights/activations appropriately"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Deep Learning frameworks take care of this for us**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In addition:\n",
    "\n",
    "- efficient matrix math for speedup.\n",
    "- use variants of gradient descent that are **much** faster/better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Major frameworks:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src = \"Images/kerastf.png\" >\n",
    "<center> Keras: high level API of Tensorflow. </center>\n",
    "\n",
    "- Keras: apt for quickly building/trying neural network architectures with standard layers/optimizers. Still build complex networks.\n",
    "- Tensorflow: for building more customized networks and more control of optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src = \"Images/pytorch.png\" >\n",
    "More customizable than Keras. More pythonic than base Tensorflow. Nice functionalities for NLP. </center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We will use Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Keras has two major neural network construction frameworks:\n",
    "- Sequential vs Functional API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Sequential API:\n",
    "\n",
    "<img src = \"Images/sequentialvsfunctional.png\" >\n",
    "\n",
    "<center>Apt for simpler feedforward network topologies.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Functional API:\n",
    "\n",
    "<center><img src = \"Images/skipconnection.png\" ></center>\n",
    "\n",
    "Skip connections, branching, etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### The Sequential API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Take in a simple and sufficiently interesting dataset:\n",
    "\n",
    "- demonstrate non-trivial decision boundary\n",
    "- show sequential API at work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# generate two moons dataset\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import make_moons\n",
    "from matplotlib import pyplot\n",
    "from pandas import DataFrame\n",
    "# generate 2d classification dataset\n",
    "X, y = make_moons(n_samples=300, noise=0.2, random_state=10)\n",
    "# scatter plot, dots colored by class value\n",
    "df = DataFrame(dict(x=X[:,0], y=X[:,1], label=y))\n",
    "\n",
    "sns.scatterplot(x = 'x', y = 'y', hue = 'label', data = df )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Import Sequential object: use Sequential API to construct models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We want to start defining layers in the model:\n",
    "    \n",
    "The most common layer type in neural networks: \n",
    "- the **densely** connected layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<img src = \"Images/dense_layer.png\" >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# creates densely connected layer object\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# defines input layer, not necessary in sequential API\n",
    "# but nice for visualizing network later\n",
    "from tensorflow.keras.layers import Input # not needed, it just helps to visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The Dense object constructor's basic arguments:\n",
    "- units: number of nodes in layer\n",
    "- activation: 'relu', 'tanh', 'sigmoid', 'softmax'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Defining the model: a shallow neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# intentiate \n",
    "model = Sequential()\n",
    "# add a layer 50- number of nodes in the 1st layer, imput dimesion - how many imputs we are taking, \n",
    "model.add(Dense(50, input_dim=2, activation='relu')) # one hidden layer\n",
    "# 2*50 + bias terms(50) = 150\n",
    "# units = 1, giving us one output, the probability for the target class (whether is in the target class or not)\n",
    "model.add(Dense(units=1, activation='sigmoid')) # binary classification - sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Building/compiling the model:\n",
    "\n",
    "- define objective function and optimizer\n",
    "- define metric to evaluate train/validation\n",
    "- build the network connections, weight matrices, initializes, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# sigmoid is binary crossentropy, multiclass is categorical crossentropy\n",
    "model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy', 'AUC'])\n",
    "# optimizer sg to do with learning rate - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Some other loss functions:\n",
    "- multiclass: 'categorical_crossentropy'\n",
    "- regression: 'mean_squared_error'\n",
    "- regression: 'mean_absolute_error'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The optimizer:\n",
    "\n",
    "- Adam Optimizer: \n",
    "    - smoothens learning process \n",
    "    - uses adaptive learning rate.\n",
    "\n",
    "A pretty good optimizer. Many other optimizers (NAdam, Adadelta)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Optimizers have hyperparameters.\n",
    "\n",
    "- Can help in training speed up / finding minimum\n",
    "\n",
    "model.compile(loss = 'binary_crossentropy', optimizer = opt, metrics = ['accuracy', 'AUC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# import Adam if you want to play with the learning rate of your optimizer\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "opt = Adam(learning_rate = 0.01) # typical parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy', 'AUC'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Metrics: takes in list (calculates metrics in list at each epoch):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Regression:\n",
    "- 'mae'\n",
    "- 'mse'\n",
    "- 'rmse'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Classification:\n",
    "- 'accuracy'\n",
    "- 'AUC'\n",
    "- 'fmeasure'\n",
    "etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Keras provides metrics callable as objects:\n",
    "- more metrics available\n",
    "- pass in arguments (probability threshold, etc.)\n",
    "- build your own metrics.\n",
    "\n",
    "See documentation for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Train test split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### .fit() method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- batch_size = # of points you train on in given forward/back-prop loop\n",
    "\n",
    "typical batch sizes: 1, 32, 64, 128, 256, 512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Batch size:\n",
    "\n",
    "- Stochastic Gradient Descent: batch_size = 1\n",
    "- Minibatch Gradient Descent: batch_size = $n_{batch} < N_{train}$\n",
    "- Batch Gradient Descent: full training set fed in on each forward/back-prop loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The intuition behind SGD, mini-batch:\n",
    "\n",
    "SGD: single point $i$\n",
    "$$ \\textbf{w}_{k+1} = \\textbf{w}_k - \\alpha \\nabla_{\\textbf{w}}L(y_i, \\hat{y}_i) $$\n",
    "\n",
    "- Using a single point may not be good enough to sample loss function and its gradient. Noisy descent. \n",
    "- Can go wrong direction.\n",
    "- Effectively looping one point at a time. Computationally inefficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Mini-batch gives you average of gradient over a sample of the training set: often good enough!\n",
    "\n",
    "$$ \\textbf{w}_{k+1} = \\textbf{w}_k - \\frac{\\alpha}{n_{batch}} \\sum_{i=1}^{n_{batch}} \\nabla_{\\textbf{w}}L(y_i, \\hat{y}_i) $$\n",
    "\n",
    "- Substantially smaller amount of data fed through: still make progress."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Batch gradient descent: all training data.\n",
    "    \n",
    "- Will always lower objective on training.\n",
    "- But depending on dataset:\n",
    "- manipulating huge matrices just to make one gradient descent step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src = \"Images/gradient_descent.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Epochs\n",
    "- Number of times to cycle through **entire** training data.\n",
    "- Implies number of forward/backprop cycles is: $$ \\frac{N_{train}}{n_{batch}} \\times \\text{epochs} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Can train through all epochs defined or:\n",
    "\n",
    "**Earlystopping callback on training**\n",
    "\n",
    "- Monitor training loss and set improvement threshold (min_delta)\n",
    "- Waiting certain number of epochs if no improvements (patience).\n",
    "- Terminate training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "trainCallback = EarlyStopping(monitor='loss', min_delta = 1e-6, patience = 5)\n",
    "# min_delta. - minimium change, stop if we don't see this size of change in 5 epoch, \n",
    "# play with how many layers, or how many nodes, before it's starts to converge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Validation set**\n",
    "\n",
    "- can feed an external validation set \n",
    "- hold out a fraction of training set for validation\n",
    "- evaluates on train and validation at each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, epochs = 10000, batch_size = 32, validation_split = 0.2, callbacks=[trainCallback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# predict method outputs probability of\n",
    "# being class 1\n",
    "y_proba = model.predict(X_test) \n",
    "y_proba[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# getting class predictions\n",
    "y_pred = (y_proba > 0.5).astype('int')\n",
    "y_pred[0:5]\n",
    "# anything greater than 0.5 is 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# _ - is the loss\n",
    "# vebose = 0 - no printout, \n",
    "_, test_accuracy, test_AUC = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(test_accuracy, test_AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "View decision boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1), np.arange(y_min, y_max, 0.1))\n",
    "\n",
    "f, ax = plt.subplots()\n",
    "\n",
    "Z = (model.predict(np.c_[xx.ravel(), yy.ravel()]) > 0.5).astype('int')\n",
    "Z = Z.reshape(xx.shape)\n",
    "ax.contourf(xx, yy, Z, alpha=0.4)\n",
    "ax.scatter(X[:, 0], X[:, 1], c = y, s=30, edgecolor=\"k\")\n",
    "ax.set_xlabel('X1')\n",
    "ax.set_ylabel('X2')\n",
    "ax.set_title('Decision Boundary: Shallow Network')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The model object returns a history method:\n",
    "- history attribute is a dictionary storing:\n",
    "   - training/validation losses.\n",
    "   - metrics on train/validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# printout \n",
    "train_accuracy = history.history['loss']\n",
    "validation_accuracy = history.history['val_loss']\n",
    "train_history = pd.DataFrame(history.history)\n",
    "train_history.index.name = 'epochs'\n",
    "train_history.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Plotting history:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "col_list = ['loss', 'val_loss']\n",
    "train_history[col_list].plot()\n",
    "plt.ylabel('Binary cross entropy')\n",
    "plt.title('Training loss history')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "col_list = ['accuracy', 'val_accuracy']\n",
    "train_history[col_list].plot()\n",
    "plt.ylabel('Classification Accuracy')\n",
    "plt.title('Training loss history')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Using neural networks to solve some harder problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Image classificaton of hand-written digits\n",
    "\n",
    "- The MNIST dataset: a classic benchmark for neural network image classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import datasets\n",
    "from tensorflow.keras.utils import to_categorical # for target labels\n",
    "from tensorflow.keras.layers import Flatten # takes 2D input and turns into 1D array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# load in the data\n",
    "(x_train, y_train), (x_test, y_test) = datasets.mnist.load_data() # the dataset already has split just has to loaded it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(9):  \n",
    "    plt.subplot(330 + 1 + i)\n",
    "    plt.imshow(x_train[i], cmap=pyplot.get_cmap('gray'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Good practice to normalize/standardize feature inputs:\n",
    "- makes learning faster (optimization doesn't depend on variable scale)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src = \"Images/normalize_nn.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Another reason:\n",
    "- keeping values small helps keeps weights stable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# 255 pixels, \n",
    "x_train = x_train/255 # normalization\n",
    "x_test = x_test/255\n",
    "\n",
    "# Keras requires multi-class labels to be one-hot encoded\n",
    "y_tr_one_hot = to_categorical(y_train)\n",
    "y_tt_one_hot = to_categorical(y_test)\n",
    "y_tt_one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Sequential model: add another hidden layer\n",
    "\n",
    "- additional layer will help us learn more complex features\n",
    "- note: we've added a few more units in first hidden layer as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model_images = Sequential()\n",
    "model_images.add(Flatten(input_shape=(28, 28))) # flattens each 28x28 image into a vector\n",
    "model_images.add(Dense(32, activation='relu'))\n",
    "model_images.add(Dense(8, activation='relu'))\n",
    "model_images.add(Dense(10, activation='softmax')) # number of units = number of target classes\n",
    "# 0-9 = 10 for the last class, it has to match the number of class we try to identify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model_images.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's train the model.\n",
    "\n",
    "- Here we will use **validation** loss: stopping to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# for early stopping check validation loss,  for accuracy the mode should be max, max accuracy\n",
    "valcallback = EarlyStopping(monitor='val_loss', mode='min', verbose = 1, patience = 2)\n",
    "# patience - how many epoc it will run before it stops "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "history_mnist = model_images.fit(x_train, y_tr_one_hot, epochs=100, batch_size= 32, validation_split = 0.2, \n",
    "                 callbacks = [trainCallback] )\n",
    "# 10 probability for the 10 classes (0-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# probability for each class\n",
    "y_proba = model_images.predict(x_test)\n",
    "y_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model_images.evaluate(x_test, y_tt_one_hot, verbose = 0)\n",
    "# accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "train_mnist_history = pd.DataFrame(history_mnist.history)\n",
    "train_mnist_history.index.name = 'epochs'\n",
    "\n",
    "col_list = ['loss', 'val_loss']\n",
    "train_mnist_history[col_list].plot()\n",
    "plt.ylabel('Categorical cross entropy')\n",
    "plt.title('Training loss history')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "col_list = ['accuracy', 'val_accuracy']\n",
    "train_mnist_history[col_list].plot()\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training loss history')\n",
    "plt.show()\n",
    "# overfiting, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Clearly, some overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Getting the class predictions from softmax probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# argmax axis = -1 gets the column index of maximum probability for each row.\n",
    "# column index corresponds to digit classes (numbers 0 -9)\n",
    "predicted = np.argmax(y_proba, axis=-1)\n",
    "predicted\n",
    "# column represent the number as well (0-9), "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's look at our classifier did on the test set in more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "cm_digits = confusion_matrix(y_test, predicted)\n",
    "disp = ConfusionMatrixDisplay(\n",
    "    confusion_matrix=cm_digits)\n",
    "\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Not bad:\n",
    "- but in the context of many tasks this is not yet a viable system.\n",
    "- e.g., reading checks, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Next steps: increasing network complexity / train longer but not overfitting\n",
    "- getting better fitting and test-set generalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><b>Network Regularization</b></center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
