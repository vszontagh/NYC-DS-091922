{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div style=\"color:white;\n",
    "           display:fill;\n",
    "           border-radius:5px;\n",
    "           background-color:#5642C5;\n",
    "           font-size:200%;\n",
    "           font-family:Arial;letter-spacing:0.5px\">\n",
    "\n",
    "<p width = 20%, style=\"padding: 10px;\n",
    "              color:white;\">\n",
    "Recommendation Systems\n",
    "              \n",
    "</p>\n",
    "</div>\n",
    "\n",
    "Data Science Cohort Live NYC Nov 2022\n",
    "<p>Phase 4: Topic 32</p>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<div align = \"right\">\n",
    "<img src=\"Images/flatiron-school-logo (1).png\" align = \"right\" width=\"200\"/>\n",
    "</div>\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![Netflix](https://miro.medium.com/max/1400/1*jlQxemlP9Yim_rWTqlCFDQ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![Amazon](images/amazon_recommender.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Agenda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- describe the difference between content-based and collaborative-filtering algorithms\n",
    "- explain and use the cosine similarity metric\n",
    "- describe the algorithm of alternating least-squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from random import gauss as gs, uniform as uni, seed\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Recommender systems can be classified along various lines. One fundamental distinction is **content-based** vs. **collaborative-filtering** systems.\n",
    "\n",
    "To illustrate this, consider two different strategies: (a) I recommend items to you that are *similar to other items* you've used/bought/read/watched; and (b) I recommend items to you that people *similar to you* have used/bought/read/watched. The first is the **content-based strategy**; the second is the **collaborative-filtering strategy**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![Content](images/Content-based-filtering-vs-Collaborative-filtering-Source.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## Collaborative-Filtering System Example\n",
    "Last.fm creates a 'station' of recommended songs by observing what bands and individual tracks the user has listened to on a regular basis and comparing those against the listening behavior of other users. This approach leverages the behavior of users.\n",
    "\n",
    "## Content-Based System Example\n",
    "Pandora uses the properties of a song or artist to see a 'station' that plays music with similar properties. User feedback is used to refine the station's results, deemphasizing certain attributes when a user 'dislikes' a particular song and emphasizing other attributes when a user 'likes' a song."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Another distinction drawn is in whether (a) the system uses existing ratings to compute user-user or item-item similarity, or (b) the system uses machine learning techniques to make predictions. Recommenders of the first sort are called **memory-based**; recommenders of the second sort are called **model-based**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Content-Based Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The basic idea here is to recommend items to a user that are *similar to* items that the user has already enjoyed. Suppose we represent TV shows as rows, where the columns represent various features of these TV shows. These features might be things like the presence of a certain actor or the show fitting into a particular genre etc. We'll just use binary features here, perhaps the result of some one-hot encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 0, 1, 1, 1],\n",
       "       [0, 0, 0, 1, 1, 1, 0],\n",
       "       [1, 1, 1, 0, 0, 1, 1],\n",
       "       [0, 1, 1, 1, 0, 0, 1]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tv_shows = np.array([[0, 1, 1, 0, 1, 1, 1],\n",
    "                   [0, 0, 0, 1, 1, 1, 0],\n",
    "                   [1, 1, 1, 0, 0, 1, 1],\n",
    "                   [0, 1, 1, 1, 0, 0, 1]])\n",
    "\n",
    "tv_shows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Troy likes the TV Show represented by Row \\#1. Which show (row) should we recommend to Harrison?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "One natural way of measuring the similarity between two vectors is by the **cosine of the angle between them**. Two points near one another in feature space will correspond to vectors that nearly overlap, i.e. vectors that describe a small angle $\\theta$. And as $\\theta$ decreases, $\\cos(\\theta)$ *increases*. So we'll be looking for large values of the cosine (which ranges between -1 and 1). We can also think of the cosine between two vectors as the *projection of one vector onto the other*:\n",
    "\n",
    "![image.png](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781788295758/files/assets/2b4a7a82-ad4c-4b2a-b808-e423a334de6f.png)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "![image.png](https://storage.googleapis.com/lds-media/images/cosine-similarity-vectors.original.jpg)\n",
    "We can use this metric easily if we treat our rows (the items we're comparing for similarity) as vectors: We can calculate the cosine of the angle $\\theta$ between two vectors $\\vec{a}$ and $\\vec{b}$ as follows: $\\cos(\\theta) = \\frac{\\vec{a}\\cdot\\vec{b}}{|\\vec{a}||\\vec{b}|}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerators:[2 4 3]\n",
      "denominators:[3.87298335 5.         4.47213595]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.51639778, 0.8       , 0.67082039])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# similarity between first row and every other row\n",
    "numerators = np.array([tv_shows[0].dot(tv_show) for tv_show in tv_shows[1:]])\n",
    "denominators = np.array([np.sqrt(sum(tv_shows[0]**2)) *\\\n",
    "                         np.sqrt(sum(tv_show**2)) for tv_show in tv_shows[1:]])\n",
    "print(f'numerators:{numerators}')\n",
    "print(f'denominators:{denominators}')\n",
    "\n",
    "numerators / denominators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Since the cosine similarity to Row \\#1 is highest for Row \\#3, we would recommend this TV show."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Collaborative Filtering\n",
    "\n",
    "Now the idea is to recommend items to a user based on what *similar* users have enjoyed. Suppose we have the following recording of explicit ratings of five items by three users:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5, 4, 3, 4, 5],\n",
       "       [3, 1, 1, 2, 5],\n",
       "       [4, 2, 3, 1, 4]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users = np.array([[5, 4, 3, 4, 5], [3, 1, 1, 2, 5], [4, 2, 3, 1, 4]])\n",
    "users\n",
    "# ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_user = np.array([5, 0, 0, 0, 0])\n",
    "new_user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Is the above explicit data or implicit data?\n",
    "\n",
    "\n",
    "To which user is `new_user` most similar?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.52414242, 0.47434165, 0.58976782])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#One metric is cosine similarity:\n",
    "\n",
    "new_user_mag = 5\n",
    "\n",
    "numerators = np.array([new_user.dot(user) for user in users])\n",
    "denominators = np.array([new_user_mag * np.sqrt(sum(user**2))\\\n",
    "                         for user in users])\n",
    "\n",
    "numerators / denominators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "But we could also use another metric, such as Pearson Correlation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5345224838248488, 0.20044593143431824, 0.5144957554275266]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[np.corrcoef(new_user, user)[0, 1] for user in users]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Use diifrent similarity metrics with your reccomendars to determine which one works best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For more on content-based vs. collaborative systems, see [this Wikipedia article](https://en.wikipedia.org/wiki/Collaborative_filtering) and [this blog post](https://towardsdatascience.com/recommendation-systems-models-and-evaluation-84944a84fb8e). [This post](https://dataconomy.com/2015/03/an-introduction-to-recommendation-engines/) on dataconomy is also useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Matrix Factorization\n",
    "\n",
    "Suppose we start with a matrix $R$ of users and products, where each cell records the ranking the relevant user gave to the relevant product. Very often we'll be able to record this data as a sparse matrix, because many users will not have ranked many items.\n",
    "\n",
    "Imagine factoring this matrix into a user matrix $P$ and an item matrix $Q^T$: $R = PQ^T$. What would the shapes of $P$ and $Q^T$ be? Clearly $P$ must have as many rows as $R$, which is just the number of users who have given ratings. Similarly, $Q^T$ must have as many columns as $R$, which is just the number of items that have received ratings. We also know that the number of columns of $P$ must match the number of rows of $Q^T$ for the factorization to be possible, but this number could really be anything. In practice this will be a small number, and for reasons that will emerge shortly let's refer to these dimensions as **latent features** of the items in $R$. If $p$ is a row of $P$, i.e. a user-vector, and $q$ is a column of $Q^T$, i.e. an item-vector, then $p$ will record the user's particular weights or *preferences* with respect to the latent features, while $q$ will record how the item ranks with respect to those same latent features. This in turn means that we could predict a user's ranking of a particular item simply by calculating the dot-product of $p$ and $q$! \n",
    "\n",
    "If we could effect such a factorization, $R = PQ^T$, then we could calculate *all* predictions, i.e. fill in the gaps in $R$, by solving for $P$ and $Q$.\n",
    "\n",
    "The isolation of these latent features can be achieved in various ways. But this is at heart a matter of **dimensionality reduction**, and so one way is with the [SVD](https://hackernoon.com/introduction-to-recommender-system-part-1-collaborative-filtering-singular-value-decomposition-44c9659c5e75).\n",
    "\n",
    "An alternative is to use the method of Alternating Least Squares."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![decomp_1](images/R_decompostion_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Users are similar becuase of items. It hightens the propbality that these items are related to each other as a group.\n",
    "We dont know what the people or item groups are but we can say that they exist and that these groups have characteristics.\n",
    "These characrisyics are the lantent factors of features that tie people and products together.\n",
    "\n",
    "We don't know how many lantent features. There hidden.\n",
    "When building a model we experiement how many lantent factors we want to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Alternating Least-Squares (ALS)\n",
    "\n",
    "ALS recommendation systems are often implemented in Spark architectures because of the appropriateness for distributed computing. ALS systems often involve very large datasets (consider how much data the recommendation engine for NETFLIX must have, for example!), and it is often useful to store them as sparse matrices, which Spark's ML library can handle. In fact, Spark's mllib even includes a \"Rating\" datatype! ALS is **collaborative** and **model-based**, and is especially useful for working with *implicit* ratings.\n",
    "\n",
    "We're looking for two matrices (a user matrix and an item matrix) into which we can factor our ratings matrix. We can't of course solve for two matrices at once. But here's what we can do:\n",
    "\n",
    "Make guesses of the values for $P$ and $Q$. Then hold the values of one *constant* so that we can optimize for the values of the other!\n",
    "\n",
    "Basically this converts our problem into a familiar *least-squares* problem. See [this page](https://textbooks.math.gatech.edu/ila/least-squares.html) and [this page](https://datasciencemadesimpler.wordpress.com/tag/alternating-least-squares/) for more details, but here's the basic idea:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Minimize the sum of the squared differences between the observed valuse the vaules on the line \n",
    "in ALS in one iteration of adjusting the values we fix the user side latent factors and adjust the products latent factors. Then we alternate by keeping the products latent factors fixed and adjusting the userlatent factors... this linear regression and grouping with kmeans."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![Filled](images/Filled_R_Matrix.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If we have an equation $Ax = b$ for *non-square* $A$, then we have:\n",
    "\n",
    "$A^TAx = A^Tb$ <br/>\n",
    "Thus: <br/>\n",
    "$x = (A^TA)^{-1}A^Tb$\n",
    "\n",
    "This $(A^TA)^{-1}A^T$ **is the pseudo-inverse of** $A$. We encountered this before in our whirlwind tour of linear algebra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "A = np.random.rand(5, 5)\n",
    "b = np.random.rand(5, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-226.1780899 ],\n",
       "       [ 218.48756743],\n",
       "       [-362.56506544],\n",
       "       [ 147.90541499],\n",
       "       [ 350.14590704]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.inv(A.T.dot(A)).dot(A.T).dot(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The `numpy` library has a shortcut for this: `numpy.linalg.pinv()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-226.17808981],\n",
       "       [ 218.48756735],\n",
       "       [-362.56506531],\n",
       "       [ 147.90541493],\n",
       "       [ 350.14590691]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.pinv(A).dot(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\"When we talk about collaborative filtering for recommender systems we want to solve the problem of our original matrix having millions of different dimensions, but our 'tastes' not being nearly as complex. Even if i’ve \\[sic\\] viewed hundreds of items they might just express a couple of different tastes. Here we can actually use matrix factorization to mathematically reduce the dimensionality of our original 'all users by all items' matrix into something much smaller that represents 'all items by some taste dimensions' and 'all users by some taste dimensions'. These dimensions are called ***latent or hidden features*** and we learn them from our data\" ([Medium article: \"ALS Implicit Collaborative Filtering\"](https://medium.com/radon-dev/als-implicit-collaborative-filtering-5ed653ba39fe))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Simple Example\n",
    "\n",
    "Suppose John and Evan have rated five films:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>film1</th>\n",
       "      <th>film2</th>\n",
       "      <th>film3</th>\n",
       "      <th>film4</th>\n",
       "      <th>film5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>John</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Evan</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      film1  film2  film3  film4  film5\n",
       "John      0      1      0      0      4\n",
       "Evan      0      0      0      5      0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_arr = pd.DataFrame([[0, 1, 0, 0, 4], [0, 0, 0, 5, 0]],\\\n",
    "                           index=['John', 'Evan'],\n",
    "             columns=['film' + str(i) for i in range(1, 6)])\n",
    "ratings_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Suppose now that we isolate ten latent features of these films, and that we can capture our users, i.e. the tastes of John and Evan, according to these features. (We'll just fill out a matrix randomly.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "seed(100)\n",
    "users = []\n",
    "\n",
    "for _ in range(2):\n",
    "    user = []\n",
    "    for _ in range(10):\n",
    "        user.append(gs(0, 1))\n",
    "    users.append(user)\n",
    "users_arr = np.array(users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.67155333,  0.87331967,  0.20361655, -1.55034921, -0.12059128,\n",
       "        -1.05927574,  0.38143697, -1.17342904,  0.96637182,  0.53248343],\n",
       "       [ 2.22041991,  0.68901284,  0.85436449, -0.29504752, -0.62335055,\n",
       "         1.5917369 ,  0.17920475,  0.60086339,  0.3474319 ,  0.85537186]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now we'll make another random matrix that expresses *our items* in terms of these latent features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "seed(100)\n",
    "items = []\n",
    "\n",
    "for _ in range(5):\n",
    "    item = []\n",
    "    for _ in range(10):\n",
    "        item.append(gs(0, 1))\n",
    "    items.append(item)\n",
    "items_arr = np.array(items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.67155333,  0.87331967,  0.20361655, -1.55034921, -0.12059128,\n",
       "        -1.05927574,  0.38143697, -1.17342904,  0.96637182,  0.53248343],\n",
       "       [ 2.22041991,  0.68901284,  0.85436449, -0.29504752, -0.62335055,\n",
       "         1.5917369 ,  0.17920475,  0.60086339,  0.3474319 ,  0.85537186],\n",
       "       [-1.80291827, -1.83311295,  0.60911087,  2.4250145 , -2.02233576,\n",
       "        -0.73382756,  0.24510831, -0.53817586, -0.30637608, -0.41367264],\n",
       "       [ 1.0027436 ,  0.03558136,  0.19013362, -1.27827915,  0.67648704,\n",
       "         1.79506722,  0.63054322, -0.37947302, -1.35033057,  0.45576721],\n",
       "       [ 0.42542416, -0.29962041, -2.48035968, -0.87457154, -1.23050164,\n",
       "        -1.00629648,  0.1857537 , -1.174924  , -0.33108494,  1.29437514]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 10)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 10)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items_arr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "To construct our large users-by-items matrix, we'll simply take the product of our two random matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7.53516384,  1.26783511, -5.21738432,  0.36547776,  3.90803828],\n",
       "       [ 1.26783511, 10.38970834, -6.10853643,  5.03189793, -1.63817674]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "users_arr.dot(items_arr.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now here's where the ALS really kicks in: We'll solve for John's and Evan's preference vectors by multiplying the pseudo-inverse of the items array by their respective ratings vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.47099573 -0.11214938 -0.98527859  0.09266666 -0.64747458  0.00854642\n",
      " -0.11601818  0.09586291 -0.08579953  0.55693307]\n",
      "[-0.23238655 -0.55138144  0.5415508  -0.71191248  0.27767197  0.81157767\n",
      "  0.78557579 -1.03623954 -1.25490079  0.02606555]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "John_pref = np.linalg.pinv(items_arr).dot(ratings_arr.loc['John', :])\n",
    "print(John_pref)\n",
    "Evan_pref = np.linalg.pinv(items_arr).dot(ratings_arr.loc['Evan', :])\n",
    "print(Evan_pref)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We'll predict (or, in this case, reproduce) John's and Evan's ratings for films by simply multiplying their preference vectors by the transpose of the items array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.,  1., -0.,  0.,  4.])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newjohn = John_pref.dot(items_arr.T)\n",
    "np.around(newjohn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.,  0., -0.,  5., -0.])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newevan = Evan_pref.dot(items_arr.T)\n",
    "np.around(newevan)\n",
    "# row * column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This lines up with the ratings with which we began."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>film1</th>\n",
       "      <th>film2</th>\n",
       "      <th>film3</th>\n",
       "      <th>film4</th>\n",
       "      <th>film5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>John</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Evan</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      film1  film2  film3  film4  film5\n",
       "John      0      1      0      0      4\n",
       "Evan      0      0      0      5      0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We'll make a quick error calculation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.888609052210118e-30\n"
     ]
    }
   ],
   "source": [
    "guess = np.vstack([newjohn, newevan])\n",
    "\n",
    "#checking for error (0)\n",
    "err = 0\n",
    "for i in range(2):\n",
    "    for j in range(len(ratings_arr.values[i, :])):\n",
    "        if ratings_arr.values[i, j] != 0:\n",
    "            err += (ratings_arr.values[i, j] - guess[i, j])**2\n",
    "print(err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Second Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# If P = users and Q = items, then we want to approximate R = PQ^T\n",
    "# Let's generate R.\n",
    "\n",
    "seed(42)\n",
    "ratings2 = []\n",
    "for _ in range(100):\n",
    "    user = []\n",
    "    for _ in range(100):\n",
    "        chance = gs(0, 0.4)\n",
    "        \n",
    "        # We'll fill our ratings matrix mostly with 0's to represent\n",
    "        # unrated films. This is NOT standard; we're doing this only\n",
    "        # to illustrate the general algorithm.\n",
    "        if chance > 0.5:\n",
    "            user.append(int(uni(1, 6)))\n",
    "        else:\n",
    "            user.append(0)\n",
    "        \n",
    "        # This 'if' will simply ensure that everyone has given at least\n",
    "        # one rating.\n",
    "        if user.count(0) == 10:\n",
    "            user[int(uni(0, 10))] = int(uni(1, 6))\n",
    "    ratings2.append(user)\n",
    "ratings_arr2 = np.array(ratings2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#P\n",
    "\n",
    "users2 = []\n",
    "\n",
    "# Random generation of values for the user matrix\n",
    "for _ in range(100):\n",
    "    user = []\n",
    "    for _ in range(10):\n",
    "        user.append(gs(0, 1))\n",
    "    users2.append(user)\n",
    "users_arr2 = np.array(users2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#Q_Transpose\n",
    "items2 = []\n",
    "\n",
    "\n",
    "# Random generation of values for the item matrix\n",
    "for _ in range(100):\n",
    "    item = []\n",
    "    for _ in range(10):\n",
    "        item.append(gs(0, 1))\n",
    "    items2.append(item)\n",
    "items_arr2 = np.array(items2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Our first guess at filling in the matrix will simply be the matrix product:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "guess = users_arr2.dot(items_arr2.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "guess.shape == ratings_arr2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's get a measure of error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.20022042, -4.79370844,  0.16745819, ...,  0.71515764,\n",
       "         5.29265957,  2.34612024],\n",
       "       [12.81954999,  1.09244517,  2.76883236, ...,  3.84048632,\n",
       "         3.63986143,  1.251285  ],\n",
       "       [-0.11924386,  0.1143655 , -0.52294795, ...,  3.37782545,\n",
       "        -2.46092459,  0.03240239],\n",
       "       ...,\n",
       "       [ 1.78409526, -0.16181232,  1.29355933, ...,  6.03771907,\n",
       "         1.97382977,  4.89026489],\n",
       "       [-1.85314949,  4.16815931,  3.84911175, ..., -0.0342376 ,\n",
       "        -5.73579374, -1.53629352],\n",
       "       [ 2.51939362,  5.31266136, -1.7857387 , ...,  5.18275941,\n",
       "        -2.55671716,  4.90783561]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_arr2 - guess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "122333.98763617325"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err = (ratings_arr2 - guess)**2\n",
    "\n",
    "np.sum(err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Pretty terrible! But we started with random numbers and have only done one iteration. Let's see if we can do better:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def als(ratings, users, items, reps=10):\n",
    "    \n",
    "    ratings_cols = ratings.T\n",
    "    for _ in range(reps):\n",
    "        new_users = []\n",
    "        for i in range(len(ratings)):\n",
    "            \n",
    "            user = LinearRegression(fit_intercept=False)\\\n",
    "            .fit(items, ratings[i]).coef_\n",
    "            new_users.append(user)\n",
    "        new_users = np.asarray(new_users)\n",
    "        \n",
    "        new_items = []\n",
    "        for i in range(len(ratings)):\n",
    "            \n",
    "            item = LinearRegression(fit_intercept=False)\\\n",
    "            .fit(new_users, ratings_cols[i]).coef_\n",
    "            new_items.append(item)\n",
    "        new_items = np.asarray(new_items)\n",
    "        \n",
    "        guess = new_users.dot(new_items.T)\n",
    "        err = 0\n",
    "        for i in range(len(ratings)):\n",
    "            for j in range(len(ratings[i])):\n",
    "                if ratings[i, j] != 0:\n",
    "                    err += (ratings[i, j] - guess[i, j])**2\n",
    "        print(err)\n",
    "        \n",
    "        items = new_items\n",
    "        \n",
    "    return new_users.dot(new_items.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We should see our error decrease with more iterations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8679.95926804698\n",
      "6511.270608291888\n",
      "6272.528821101911\n",
      "6190.959718973223\n",
      "6153.148460896033\n",
      "6133.158269096349\n",
      "6121.902204676268\n",
      "6115.079057339579\n",
      "6110.538074584968\n",
      "6107.255266508423\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.74194747,  1.80370167, -0.01231969, ..., -0.52264462,\n",
       "        -0.17065981, -0.04069705],\n",
       "       [ 2.9866113 ,  0.49864947,  1.83461934, ...,  0.88047052,\n",
       "         0.04379946,  0.94685878],\n",
       "       [ 6.43813128,  3.12012023,  0.14857435, ...,  0.45034766,\n",
       "         0.63562095,  0.9139252 ],\n",
       "       ...,\n",
       "       [ 0.83785533,  0.44888671,  1.07036379, ...,  1.09727237,\n",
       "         0.26665272,  0.57135139],\n",
       "       [ 0.01327197,  1.47679197,  4.47224865, ...,  0.9471664 ,\n",
       "         0.3863458 ,  0.08476563],\n",
       "       [ 1.88904546,  1.00613493,  0.61554362, ...,  1.87402714,\n",
       "        -0.43890869,  1.15908189]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "als(ratings_arr2, users_arr2, items_arr2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### ALS in `pyspark`\n",
    "\n",
    "We'll talk about Big Data and Spark soon, but I'll just note here that Spark has a recommendation submodule inside its ml (machine learning) module. Source code for `pyspark`'s version [here](https://spark.apache.org/docs/latest/api/python/_modules/pyspark/ml/recommendation.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Exit Ticket\n",
    "1. Describe the difference between content-based and collaborative-filtering algorithms.\n",
    "2. Explain and use the cosine similarity metric. (-1 to 1: 1 similar, -1 oposit)\n",
    "3. Describe the algorithm of alternating least-squares.\n",
    "\n",
    "Bonus:\n",
    "what is the difference between implicit and explicit data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
