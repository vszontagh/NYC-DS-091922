{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div style=\"color:white;\n",
    "           display:fill;\n",
    "           border-radius:5px;\n",
    "           background-color:#5642C5;\n",
    "           font-size:200%;\n",
    "           font-family:Arial;letter-spacing:0.5px\">\n",
    "\n",
    "<p width = 20%, style=\"padding: 10px;\n",
    "              color:white;\">\n",
    "Principal Component Analysis\n",
    "              \n",
    "</p>\n",
    "</div>\n",
    "\n",
    "Data Science Cohort Live NYC Nov 2022\n",
    "<p>Phase 4: Topic 31</p>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<div align = \"right\">\n",
    "<img src=\"Images/flatiron-school-logo.png\" align = \"right\" width=\"200\"/>\n",
    "</div>\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.special import gamma\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels as sm\n",
    "from statsmodels.regression.linear_model import OLS\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "\n",
    "# this will allow us to visualize the pipeline (may not be available in learn-env)\n",
    "from sklearn import set_config\n",
    "set_config(display= 'diagram')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Learning Goals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Explain the concepts behind principal component analysis (PCA)\n",
    "- Explain how PCA addresses the problem of multicollinearity\n",
    "- Explain the idea of eigendecomposition\n",
    "- Implement PCA using `sklearn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Motivation: The curse of dimensionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "%%capture cod\n",
    "\n",
    "def sphere_calc(m, R):\n",
    "    numerator = (R**m * np.pi**(m/2))\n",
    "    denom = gamma(m/2 + 1)\n",
    "    \n",
    "    return numerator/denom\n",
    "\n",
    "dim_list = pd.Series(np.arange(1,21))\n",
    "sph_vol_list = dim_list.map(lambda M: sphere_calc(M, 2)) # get +- 2 std of standardized variables\n",
    "\n",
    "density_vs_dim = 5000/sph_vol_list\n",
    "fig, ax = plt.subplots()\n",
    "sns.lineplot(x = density_vs_dim.index, y = density_vs_dim.values, ax = ax)\n",
    "ax.set_ylabel('Point density')\n",
    "ax.set_xlabel('Number of Features')\n",
    "ax.set_title('Point density:  feature dimension scaling')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "- Need good statistical sampling in feature space for training\n",
    "- Dataset becomes sparse in high dimension. Hard to do statistical learning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "cod()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Small numbers (low density):\n",
    "\n",
    "- large sample-to-sample fluctuations.\n",
    "\n",
    "High variance from dimensionality: hard to train model that generalizes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Another issue in data with a ton of features:\n",
    "- many of the features are likely correlated with each other\n",
    "- high variance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How could we address this problem?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Could keep all variables and regularize.\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Or could figure out how to reduce dimension:\n",
    "\n",
    "- construct smaller set of variables that are combinations of original variables\n",
    "- largely account for variation scale and correlations of features with this smaller set\n",
    "- i.e., reduce dim, preserve as much information as possible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Principal Component Analysis (PCA) is a tool for reducing the dimensionality of our data in a way that tries to preserve information. It does this by projecting our data from a higher-dimensional space onto a lower-dimensional space. The PCA algorithm chooses a lower-dimensional space to project to that will preserve as much variance as possible from our original dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src = \"Images/PCA_basics.gif\" width = 800 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Rotate to new coordinate system: \n",
    "    - directions characteristic of data spread in various dimensions.\n",
    "    - new features uncorrelated (perpendicular)\n",
    "- Find direction(s) of maximum variance:\n",
    "    - project data onto this lower-D space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Often doing this in much higher-D spaces.\n",
    "- Very surprising how low the dimensionality capturing most of the feature variance can be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's try an implement within given situation:\n",
    "\n",
    "- Predict the cost to ship a package based on its features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center",
    "hidden": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "packages = pd.read_csv('data/packages.csv')\n",
    "packages.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "packages.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Quite a few features for a small dataset,\n",
    "- Also: likely that physical dimensions are correlated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Check out those correlations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "packages_scaled = (packages - packages.mean())/packages.std()\n",
    "\n",
    "sns.heatmap(packages_scaled.corr(),\n",
    "            annot=True,\n",
    "            fmt='0.2g',\n",
    "            vmin=-1,\n",
    "            vmax=1,\n",
    "            center=0,\n",
    "            cmap='coolwarm');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Very high correlations between the different physical package characteristics.\n",
    "\n",
    "- Some combination of variables as one variable\n",
    "- Another set for the other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- One as strong mixture of physical features\n",
    "- other dominated by Distance.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Dimensionality\n",
    "You can think about each variable as a dimension, and thus each package as a data point. If we take just one feature, we can easily visualize this in 2 dimensional space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Visualizing: looks like a linear regression might work.\n",
    "- But don't drop all features except length\n",
    "- More principled way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "packages.plot(kind='scatter', y='Shipping Cost ($)', x='Length (in)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "You can think of each package as a point in six-dimensional space - 5 dimensions for our features and 1 for our target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X = packages.drop(columns = ['Shipping Cost ($)'])\n",
    "y = packages['Shipping Cost ($)']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now standardize our features:\n",
    "- Failing to do so makes finding directions of variance:\n",
    "    - dependent on scale of individual features\n",
    "- Want to understand directions of high data variance unbiased /by the scale of individual features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_tr_norm = scaler.fit_transform(X_train)\n",
    "X_tt_norm = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "PCA: \n",
    "- relies on decomposing this **covariance matrix** $X^TX$ of scaled/centered data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "N_tr = X_tr_norm.shape[0]\n",
    "Xcov = (X_tr_norm.T@X_tr_norm)/N_tr \n",
    "Xcov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This really nothing more than the Pearson correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X_train.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "So first step of PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Transform coordinates: constructs new features\n",
    "\n",
    "Linear combination of old features:\n",
    "\n",
    "$$ \\begin{bmatrix} PC_1 \\\\ PC_2 \\end{bmatrix} = S \\begin{bmatrix} X_1 \\\\ X_2 \\end{bmatrix} = \\begin{bmatrix} s_{11} & s_{12} \\\\ s_{21} & s_{22} \\end{bmatrix}\\begin{bmatrix} X_1 \\\\ X_2 \\end{bmatrix}   $$\n",
    "\n",
    "\n",
    "$$ \\begin{bmatrix} PC_1 \\\\ PC_2 \\end{bmatrix} = X_1 \\begin{bmatrix} s_{11} \\\\ s_{12} \\end{bmatrix} + X_2 \\begin{bmatrix} s_{21} \\\\ s_{22} \\end{bmatrix}   $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$ \\begin{bmatrix} PC_1 \\\\ PC_2 \\end{bmatrix} = X_1 \\textbf{s}_1 + X_2\\textbf{s}_2  $$\n",
    "\n",
    "where $\\textbf{s}_1$ and $\\textbf{s}_2$ forms new coordinate system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src = \"Images/PCA_closeup.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In rotated frame:\n",
    "    \n",
    "- New features not correlated.\n",
    "\n",
    "<img src = \"Images/pca_rot.jpg\" width = 350 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Getting variance matrix in new coordinate system:\n",
    "- Get variance along specific PC1, PC2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Task is two fold:\n",
    "- find transformation that yields the principal axes of variation in data\n",
    "- get variance of data in these principal directions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Numpy:**\n",
    "\n",
    "Finds characteristic (eigen) directions of variance and gets (eigen) values of the variance along characteristic directions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from numpy import linalg\n",
    "eig, eigvecs = linalg.eig(Xcov)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Eigendecomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We will use an **eigendecomposition** of the covariance matrix to create a new set of dimensions. We can then decide how many of these dimensions to keep based on how much variance is captured by each dimension.\n",
    "\n",
    "Here, we show you how to do this using the NumPy `.eig()` function, but we will learn how to do PCA more easily in `sklearn` later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Eigenvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Eigenvalues** represent the relative amount of variance captured by each new dimension. The average eigenvalue will be 1, so we look for values over 1 to identify dimensions that capture more variance than average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "eig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Covariance matrix in new coordinate system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "colname = ['PC' + str(n) for n in range(len(Xcov)) ]\n",
    "Xcov_PCA = pd.DataFrame(np.diag(eig), index = colname, columns = colname)\n",
    "Xcov_PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Proportion of Variance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "You can also divide your eigenvalues by the number of features and then interpret them as the _proportion of variance in the features_ captured by each dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "eig/5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Covariance matrix in new, rotated coordinate system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "It looks like we have one great dimension capturing 3.4x more variance than average, one OK dimension capturing an average amount of variance, and three other dimensions that don't capture much variance. This is in line with what we were expecting! It means that we can just use the first two dimensions - and drop the last three - without losing much variance/information from our predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Eigenvectors (aka Principal Components)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Eigenvectors** represent the new dimensions, which we call **principal components** when doing PCA. There is one eigenvector for each dimension, and they are all combined together into one matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "eigvecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The components of the principal directions in terms of original features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pc_comp = pd.DataFrame(eigvecs.T,\n",
    "                       index = colname,\n",
    "                       columns = X_train.columns)\n",
    "pc_comp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In PCA, the values in our eigenvectors are called **component weights**, and they tell us how much variance of each feature is captured by that dimension. These weights range from -1 to 1, but the relative sizes are what matter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "PCs are mixtures of original features:\n",
    "- In our case very equally distributed across different features.\n",
    "- PCs are usually NOT interpretable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Looking at the variance and principal components side-by-side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "Xcov_PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pc_comp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "No question that PC0 and PC1 dominate variance:\n",
    "\n",
    "- Most dominant always called **first principal component**\n",
    "- Second most dominant: **second principal component**, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- First principal component has high weight across all features.\n",
    "- Second weights distance (not physical package attribute)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Together enough to capture most variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Orthogonality: formal way of saying something we know"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "hidden": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The directions we found are **orthogonal** or perpendicular to each other:\n",
    "- Same as saying: PCs are not correlated with each other.\n",
    "- In such a case: dot products between different PC vectors vanish.\n",
    "\n",
    "$$ \\textbf{u}\\cdot \\textbf{v} = \\textbf{u}^T\\textbf{v} = \\begin{bmatrix} u_1 & u_2 \\end{bmatrix}\\begin{bmatrix} v_1 \\\\ v_2 \\end{bmatrix}  = 0 $$\n",
    "\n",
    "$$ \\begin{bmatrix} 1 & -1 \\end{bmatrix}\\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}  = 0 $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src = \"Images/pca_rot.jpg\" width = 350 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What we did:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src = \"Images/PCA_orth.png\" width = 450 /></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "hidden": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "PC1 = eigvecs[:, 0]\n",
    "PC2 = eigvecs[:, 1]\n",
    "PC1.dot(PC2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### The final step: transforming the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "hidden": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We will now use these principal components to create new features. \n",
    "- These features will be weighted sums (aka **linear combinations**) of existing features\n",
    "- Using the component weights from the eigenvectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Take first three PCs only: these **dominate** variance:\n",
    "- Capture most of feature space.\n",
    "- Decorrelated variables. Variance not an issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We will now create feature using the first 3 PCS: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "firstthree = pc_comp.iloc[0:3]\n",
    "firstthree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src = \"Images/PCA_basics.gif\" width = 500 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Projecting the data onto these principal components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "projected = X_tr_norm@firstthree.T\n",
    "\n",
    "projected.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- We have lessened collinearity issue.\n",
    "- Also reduced dimensionality considerably."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sns.heatmap(projected.corr(),\n",
    "            annot=True,\n",
    "            fmt='0.2g',\n",
    "            vmin=-1,\n",
    "            vmax=1,\n",
    "            center=0,\n",
    "            cmap= 'coolwarm');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Now let's compare linear regression models with...\n",
    "\n",
    "* All five original features \n",
    "* Only 2 best new features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Model 1: All base features**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sm1 = LinearRegression()\n",
    "sm1.fit(X_tr_norm, y_train)\n",
    "sm1.score(X_tr_norm, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "y_test_hat = sm1.predict(X_tt_norm)\n",
    "r2_score(y_test, y_test_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Model 2: Major principal components**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sm3 = LinearRegression()\n",
    "sm3.fit(projected, y_train) # takes in PCA-ed + projected features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "PCA transform scaled test data:\n",
    "- Feed in to make prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "projected_test = X_tt_norm@firstthree.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#transformed train set\n",
    "y_train_pred = sm3.predict(projected) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    " # transformed test set \n",
    "y_pred = sm3.predict(projected_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "r2_score(y_train_pred, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "r2_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## PCA in `sklearn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "As always, `sklearn` makes this all much easier, this time with the `PCA()` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=3) # Check out how `n_components` works\n",
    "\n",
    "# fits PCA (stores eigenvalues/eigenvectors)\n",
    "# applies PCA transformation/projection on scaled training data\n",
    "X_tr_pca = pca.fit_transform(X_tr_norm) \n",
    "\n",
    "X_tr_pca[:5,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Saved parameters:\n",
    "- **Get eigenvectors determined from covariance matrix of training data**\n",
    "- Get eigenvalues: variance of data in different principal directions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pca.components_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- These are components of PC vectors in original feature space.\n",
    "- Sometimes the signs get flipped on the eigenvectors - don't worry about it. Think of \"up\" and \"down\" as both representing the same dimension, just in opposite directions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Saved parameters:\n",
    "- Get eigenvectors determine from covariance matrix of training data\n",
    "- **Get eigenvalues of covariance matrix of training data:**\n",
    "   - **variance of data in different principal directions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pca.explained_variance_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Often useful to see percentage of total variance explained by each PC:\n",
    "- percentage of eigenvalue (i.e., variance) of given PC\n",
    "- with respect to total variance of the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "After PCA, you can use your transformed data as you would in any model:\n",
    "- PCA acts as a transformer in the pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "steps = [('scaler', StandardScaler()), ('pca', PCA(n_components = 3)), ('model', LinearRegression())  ]\n",
    "mod_pipe = Pipeline(steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Fitting the model on unscaled data:\n",
    "- fits/transforms scaler\n",
    "- PCA and transform scalef train set\n",
    "- PCA-ed features sent to model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "mod_pipe.fit(X_train,y_train) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now evaluate:\n",
    "- train set performance\n",
    "- test set generalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "mod_pipe.score(X_train,y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#test_pred = mod_pipe.predict(X_test)\n",
    "#r2_score(y_test,test_pred)\n",
    "# or\n",
    "mod_pipe.score(X_test,y_test) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "May need to tune n_components using grid-searching:\n",
    "- but EDA can usually help a lot with this\n",
    "- get eigenvalues of scaled/centered variance matrix and determine how many principal components are significant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Scenario: Car Properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Use PCA to reduce the dimensionality of features in the example below: Predict car mpg using car properties. We've done the data prep. Now you practice the modeling, including scoring on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "cars = pd.read_csv('data/cars.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "cars.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "cars.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "cars[' cubicinches'].replace(' ', np.nan, inplace=True)\n",
    "cars[' cubicinches'] = cars[' cubicinches'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "cars[' weightlbs'].replace(' ', np.nan, inplace=True)\n",
    "cars[' weightlbs'] = cars[' weightlbs'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(cars.drop('mpg', axis=1),\n",
    "                                                    cars['mpg'],\n",
    "                                                   random_state=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "ct1 = ColumnTransformer(transformers=[\n",
    "    ('imputer', SimpleImputer(), [1, 3])],\n",
    "    remainder='passthrough')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "ct2 = ColumnTransformer(transformers=[\n",
    "    ('scaler', StandardScaler(), [0, 1, 2, 3, 4, 5]),\n",
    "    ('ohe', OneHotEncoder(), [6])],\n",
    "    remainder='passthrough')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pipe = Pipeline(steps=[\n",
    "    ('ct1', ct1),\n",
    "    ('ct2', ct2)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "pipe.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X_tr_pp = pipe.transform(X_train)\n",
    "X_te_pp = pipe.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## First Model w/o PCA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## Let's start with a linear regression\n",
    "\n",
    "lr = LinearRegression().fit(X_tr_pp, y_train)\n",
    "\n",
    "## Score on train\n",
    "\n",
    "lr.score(X_tr_pp, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## Score on test\n",
    "\n",
    "lr.score(X_te_pp, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Get the coefficients of the best-fit hyperplane\n",
    "\n",
    "lr.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Thus, our best-fit hyperplane is given by:\n",
    "\n",
    "$2.177\\times in^3\\_sd - 4.645\\times lbs.\\_sd - 1.555\\times cyl\\_sd - 1.154\\times hp\\_sd -  0.267\\times time_{60}\\_sd + 2.604\\times yr\\_sd + 0.708\\times brand_{Europe} + 0.912\\times brand_{Japan} - 1.620\\times brand_{US}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model w PCA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "cars_pca = PCA(n_components=3) \n",
    "\n",
    "X_train_new = cars_pca.fit_transform(X_tr_pp)\n",
    "X_test_new = cars_pca.transform(X_te_pp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "cars_pca.components_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The results of our PCA are as follows:\n",
    "\n",
    "**PC1** = 0.465 * cubicinches_sd + 0.435 * weightlbs_sd + 0.449 * cylinders_sd + 0.454 * hp_sd - 0.349 * time-to-60_sd - 0.187 * year_sd - 0.068 * Europe - 0.073 * Japan + 0.140 * US\n",
    "\n",
    "**PC2** = -0.099 * cubicinches_sd - 0.196 * weightlbs_sd - 0.131 * cylinders_sd + 0.006 * hp_sd - 0.125 * time-to-60_sd - 0.937 * year_sd + 0.129 * Europe + 0.022 * Japan - 0.152 * US\n",
    "\n",
    "**PC3** = 0.141 * cubicinches_sd + 0.342 * weightlbs_sd + 0.187 * cylinders_sd - 0.144 * hp_sd + 0.851 * time-to-60_sd - 0.239 * year_sd + 0.043 * Europe - 0.132 * Japan + 0.089 * US"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Modeling with New Dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Now that we have optimized our features, we can build a new model with them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "lr_pca = LinearRegression()\n",
    "lr_pca.fit(X_train_new, y_train)\n",
    "lr_pca.score(X_train_new, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X_test_new = cars_pca.transform(X_te_pp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "lr_pca.score(X_test_new, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "lr_pca.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, our best-fit hyperplane is given by:\n",
    "\n",
    "$-2.967\\times PC1 - 1.162\\times PC2 -2.486\\times PC3$\n",
    "\n",
    "Of course, since the principal components are just linear combinations of our original predictors, we could re-express this hyperplane in terms of those original predictors!\n",
    "\n",
    "And if the PCA was worth anything, we should expect the new linear model to be *different from* the first!\n",
    "\n",
    "Recall that we had:\n",
    "\n",
    "**PC1** = 0.465 * cubicinches_sd + 0.435 * weightlbs_sd + 0.449 * cylinders_sd + 0.454 * hp_sd - 0.349 * time-to-60_sd - 0.187 * year_sd - 0.068 * Europe - 0.073 * Japan + 0.140 * US\n",
    "\n",
    "**PC2** = -0.099 * cubicinches_sd - 0.196 * weightlbs_sd - 0.131 * cylinders_sd + 0.006 * hp_sd - 0.125 * time-to-60_sd - 0.937 * year_sd + 0.129 * Europe + 0.022 * Japan - 0.152 * US\n",
    "\n",
    "**PC3** = 0.141 * cubicinches_sd + 0.342 * weightlbs_sd + 0.187 * cylinders_sd - 0.144 * hp_sd + 0.851 * time-to-60_sd - 0.239 * year_sd + 0.043 * Europe - 0.132 * Japan + 0.089 * US\n",
    "\n",
    "Therefore, our new PCA-made hyperplane can be expressed as:\n",
    "\n",
    "$-2.967\\times(0.465\\times in^3\\_sd + 0.435\\times lbs.\\_sd + 0.449\\times cyl\\_sd + 0.454\\times hp\\_sd - 0.349\\times time_{60}\\_sd - 0.187\\times yr\\_sd - 0.068\\times brand_{Europe} - 0.073\\times brand_{Japan} + 0.140\\times brand_{US})$ <br/> $- 1.162\\times(-0.099\\times in^3\\_sd - 0.196\\times lbs.\\_sd - 0.131\\times cyl\\_sd + 0.006\\times hp\\_sd - 0.125\\times time_{60}\\_sd - 0.937\\times yr\\_sd + 0.129\\times brand_{Europe} + 0.022\\times brand_{Japan} - 0.152\\times brand_{US})$ <br/> $- 2.486\\times(0.141\\times in^3\\_sd + 0.342\\times lbs.\\_sd + 0.187\\times cyl\\_sd -0.144\\times hp\\_sd + 0.851\\times time_{60}\\_sd - 0.239\\times yr\\_sd + 0.043\\times brand_{Europe} - 0.132\\times brand_{Japan} + 0.089\\times brand_{US})$\n",
    "\n",
    "Let's make these calculations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print(f'cubicinches_sd coef: {round(lr_pca.coef_ @ cars_pca.components_[:, 0], 3)}')\n",
    "print(f'weightlbs_sd coef: {round(lr_pca.coef_ @ cars_pca.components_[:, 1], 3)}')\n",
    "print(f'cylinders_sd coef: {round(lr_pca.coef_ @ cars_pca.components_[:, 2], 3)}')\n",
    "print(f'horsepower_sd coef: {round(lr_pca.coef_ @ cars_pca.components_[:, 3], 3)}')\n",
    "print(f'timeto60_sd coef: {round(lr_pca.coef_ @ cars_pca.components_[:, 4], 3)}')\n",
    "print(f'year_sd coef: {round(lr_pca.coef_ @ cars_pca.components_[:, 5], 3)}')\n",
    "print(f'Europe coef: {round(lr_pca.coef_ @ cars_pca.components_[:, 6], 3)}')\n",
    "print(f'Japan coef: {round(lr_pca.coef_ @ cars_pca.components_[:, 7], 3)}')\n",
    "print(f'US coef: {round(lr_pca.coef_ @ cars_pca.components_[:, 8], 3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "So our best-fit hyperplane using PCA is:\n",
    "\n",
    "$-1.616\\times in^3\\_sd - 1.913\\times lbs.\\_sd - 1.646\\times cyl\\_sd - 0.996\\times hp\\_sd - 0.933\\times time_{60}\\_sd + 2.237\\times yr\\_sd - 0.055\\times brand_{Europe} + 0.517\\times brand_{Japan} - 0.462\\times brand_{US}$\n",
    "\n",
    "Recall that our first linear regression model had:\n",
    "\n",
    "$2.177\\times in^3\\_sd - 4.645\\times lbs.\\_sd - 1.555\\times cyl\\_sd - 1.154\\times hp\\_sd -  0.267\\times time_{60}\\_sd + 2.604\\times yr\\_sd + 0.708\\times brand_{Europe} + 0.912\\times brand_{Japan} - 1.620\\times brand_{US}$\n",
    "\n",
    "which is clearly a different hyperplane."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "dplearn",
   "language": "python",
   "name": "dplearn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "294px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
